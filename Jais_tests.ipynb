{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_rX2AZs4m2H",
        "outputId": "b390fd04-f278-41f0-a24a-45ab9823369e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install torch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POfXv3kI4cab"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model_path = \"inception-mbzuai/jais-13b\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True, offload_folder=\"offload\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpCgJ1xh97Gs"
      },
      "outputs": [],
      "source": [
        "def get_response(text,tokenizer=tokenizer,model=model):\n",
        "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "    inputs = input_ids.to(device)\n",
        "    input_len = inputs.shape[-1]\n",
        "\n",
        "    generate_ids = model.generate(\n",
        "        inputs,\n",
        "        top_p=0.9,\n",
        "        temperature=0.3,\n",
        "        max_length=200-input_len,\n",
        "        min_length=input_len + 4,\n",
        "        repetition_penalty=1.2,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    response = tokenizer.batch_decode(\n",
        "        generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "    )[0]\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtasQbsPEwcB"
      },
      "source": [
        "## Basic Model inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1rS90KkZ4gnm",
        "outputId": "ecaed4ac-7e3e-4c13-ab4c-35834e1e08d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "عاصمة دولة الإمارات العربية المتحدة هيمدينة أبوظبي, وهي مدينة عصرية تتميز بناطحات السحاب الحديثة. أما دبي فهي مركز الأعمال في البلاد, كما أنها تضم أعلى مبنى في العالم وهو برج خليفة الذي يبلغ ارتفاعه 828 مترا.\n",
            "Arabic Response inference time:  398.2313  Seconds\n",
            "==============================\n",
            "The capital of UAE is  Abu Dhabi.\n",
            "\n",
            "Geography and climate \n",
            "\n",
            "Abu Dhabi has a hot desert climate (Köppen: BWh) with long, very dry summers and short winters that are warm to moderately cool; the average annual temperature in Abu Dhabi city is. The hottest month on record was June 2018 when the mean maximum daily air temperature reached, while the coldest month was January 2019 at. Temperatures above  occur for an average of 15 days per year, while temperatures below freezing only happen once every three years or so. Annual rainfall averages around  but can be as low as zero during droughts such as the one which occurred between 2007–2009. Most precipitation occurs from October through March, averaging about  annually. In summer, daytime highs often exceed  and nighttime lows reach into the mid-teens Celsius. During winter months,\n",
            "English Response inference time:  1711.3801  Seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# English prediction\n",
        "text= \"عاصمة دولة الإمارات العربية المتحدة ه\"\n",
        "start = time.time()\n",
        "arabic_response = get_response(text)\n",
        "print(arabic_response)\n",
        "print(\"Arabic Response inference time: \", round(time.time()-start,4), \" Seconds\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Arabic prediction\n",
        "start = time.time()\n",
        "text = \"The capital of UAE is \"\n",
        "English_response = get_response(text)\n",
        "print(English_response)\n",
        "print(\"English Response inference time: \", round(time.time()-start,4), \" Seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pwXq6jKMahu"
      },
      "source": [
        "## Model Arch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_XAc8vyNkVP",
        "outputId": "fb05aa72-48f1-4b4f-9771-2d2952637a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable model parameters: 13020811960\n",
            "All model parameters: 13020811960\n",
            "Percentage of trainable model parameters: 100.00%\n"
          ]
        }
      ],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"Trainable model parameters: {trainable_model_params}\\nAll model parameters: {all_model_params}\\nPercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKcr6_u9x6bt"
      },
      "source": [
        "# Layer replacement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRhZEUciWdz7",
        "outputId": "0a8309f9-c856-455f-ec56-6b8775cc9a2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lm_head Linear(in_features=5120, out_features=84992, bias=False)\n",
            "lm_head <bound method Module.parameters of Linear(in_features=5120, out_features=84992, bias=False)>\n"
          ]
        }
      ],
      "source": [
        "for k, m in model.named_modules():\n",
        "  if type(m).__name__ == 'Linear':\n",
        "    print(k,m)\n",
        "    print(k,m.parameters)\n",
        "  else:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqujJ_LLA9tb",
        "outputId": "a57ebb7c-974c-4861-b70e-9800e9489f9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lm_head Linear(in_features=5120, out_features=84992, bias=False)\n",
            "lm_head <bound method Module.parameters of Linear(in_features=5120, out_features=84992, bias=False)>\n"
          ]
        }
      ],
      "source": [
        "for k, m in model.named_modules():\n",
        "  if type(m).__name__ == 'Linear':\n",
        "    print(k,m)\n",
        "    print(k,m.parameters)\n",
        "  else:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWWRJXO3y7wr"
      },
      "outputs": [],
      "source": [
        "for name, layer in model.named_modules():\n",
        "    if isinstance(layer, torch.nn.Linear):\n",
        "      model._modules[name] = torch.nn.Linear(5120, 84992, bias = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx8_3Vj40WUz"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQvBQrNuWfOh"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-86mK3Ux33g",
        "outputId": "16f6dd02-dc1d-4160-e88a-e7d2b2f94a56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable model parameters: 13456055992\n",
            "All model parameters: 13456055992\n",
            "Percentage of trainable model parameters: 100.00%\n"
          ]
        }
      ],
      "source": [
        "print(print_number_of_trainable_model_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zjm7XgOxOJw"
      },
      "source": [
        "# Complete Methodolgy\n",
        "\n",
        "\n",
        "\n",
        "1.   Read Model\n",
        "2.   Freeze All layers\n",
        "3.   Put my Layer (RBF)\n",
        "4.   Fine tune only that layer on summarization\n",
        "5.   Fine tune the original\n",
        "6.   Compare accuracy (ROUGE) and training time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50xi2nR0YfrV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}